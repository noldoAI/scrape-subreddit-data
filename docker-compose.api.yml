version: "3.8"

services:
  reddit-scraper-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: reddit-scraper-api
    ports:
      - "8000:8000"
    env_file:
      - .env
    restart: unless-stopped
    group_add:
      - "114"  # Docker group GID
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      # Mount the current directory so the API can access posts_scraper.py and comments_scraper.py
      - .:/app
      # Mount Docker socket to allow API to manage other containers
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PYTHONPATH=/app
      - DOCKER_HOST=unix:///var/run/docker.sock
    depends_on:
      - reddit-scraper-builder

  # Build the reddit-scraper image that the API will use to create posts/comments scraper containers
  reddit-scraper-builder:
    build:
      context: .
      dockerfile: Dockerfile
    image: reddit-scraper
    command: echo "Image built successfully"
    restart: "no"

  # Cleanup dangling images after builds
  cleanup:
    image: docker:cli
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: sh -c "docker image prune -f && echo 'Dangling images cleaned up'"
    depends_on:
      - reddit-scraper-builder
      - reddit-scraper-api
    restart: "no"
